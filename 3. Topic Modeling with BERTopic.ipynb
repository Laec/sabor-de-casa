{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fbbad0",
   "metadata": {},
   "source": [
    "## Text cleanup process\n",
    "\n",
    "    Text cleanup: remove special characters, numbers, punctuation, and unnecessary white space.\n",
    "\n",
    "    Eliminate stopwords: eliminate common words that do not add meaning to the analysis, such as \"de\", \"la\", \"que\", etc.\n",
    "\n",
    "    Remove URLs and usernames: remove urls and usernames from posts\n",
    "\n",
    "    Remove hashtags: remove hashtags from posts\n",
    "\n",
    "    Remove emoticons: remove emoticons from posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0498e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313bc745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>image</th>\n",
       "      <th>video</th>\n",
       "      <th>video_watches</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>...</th>\n",
       "      <th>links</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>reaction_count</th>\n",
       "      <th>likes_standar</th>\n",
       "      <th>comments_standar</th>\n",
       "      <th>shares_standar</th>\n",
       "      <th>dia</th>\n",
       "      <th>hora</th>\n",
       "      <th>total_reacciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10160500185237520</td>\n",
       "      <td>Jam√°s habr√° pierde en un jugosito #s√°nguche de...</td>\n",
       "      <td>2023-02-01 13:23:02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.296142e+11</td>\n",
       "      <td>LA LUCHA SANGUCHERIA CRIOLLA</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.010323</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>13</td>\n",
       "      <td>0.034773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10160496038652520</td>\n",
       "      <td>Fuimos en b√∫squeda del mejor planchero üòùüí™üèª ¬øqu...</td>\n",
       "      <td>2023-01-30 13:07:26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.296142e+11</td>\n",
       "      <td>LA LUCHA SANGUCHERIA CRIOLLA</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.007358</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>Monday</td>\n",
       "      <td>13</td>\n",
       "      <td>0.032418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10160489394752520</td>\n",
       "      <td>Y si est√°s con hambre, nuestro #pollo deluxe s...</td>\n",
       "      <td>2023-01-27 09:20:29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.296142e+11</td>\n",
       "      <td>LA LUCHA SANGUCHERIA CRIOLLA</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>Friday</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10160481576377520</td>\n",
       "      <td>Te haremos antojar un s√°nguche de pavo en 3,2,...</td>\n",
       "      <td>2023-01-23 10:14:42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.296142e+11</td>\n",
       "      <td>LA LUCHA SANGUCHERIA CRIOLLA</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10160476026507520</td>\n",
       "      <td>Come desde S/5.90 en La Lucha Sangucher√≠a ü§©‚ú®</td>\n",
       "      <td>2023-01-20 18:36:27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>567.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.296142e+11</td>\n",
       "      <td>LA LUCHA SANGUCHERIA CRIOLLA</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>Friday</td>\n",
       "      <td>18</td>\n",
       "      <td>0.159266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            post_id  \\\n",
       "0           0  10160500185237520   \n",
       "1           1  10160496038652520   \n",
       "2           2  10160489394752520   \n",
       "3           3  10160481576377520   \n",
       "4           4  10160476026507520   \n",
       "\n",
       "                                                text                 time  \\\n",
       "0  Jam√°s habr√° pierde en un jugosito #s√°nguche de...  2023-02-01 13:23:02   \n",
       "1  Fuimos en b√∫squeda del mejor planchero üòùüí™üèª ¬øqu...  2023-01-30 13:07:26   \n",
       "2  Y si est√°s con hambre, nuestro #pollo deluxe s...  2023-01-27 09:20:29   \n",
       "3  Te haremos antojar un s√°nguche de pavo en 3,2,...  2023-01-23 10:14:42   \n",
       "4       Come desde S/5.90 en La Lucha Sangucher√≠a ü§©‚ú®  2023-01-20 18:36:27   \n",
       "\n",
       "   image  video  video_watches  likes  comments  shares  ... links  \\\n",
       "0    1.0      1            NaN  121.0      19.0     8.0  ...    10   \n",
       "1    1.0      1            NaN   64.0      39.0     5.0  ...    10   \n",
       "2    1.0      1            NaN   81.0      12.0     2.0  ...     9   \n",
       "3    1.0      1            NaN   85.0      11.0     2.0  ...    10   \n",
       "4    1.0      1            NaN  567.0      67.0    40.0  ...     1   \n",
       "\n",
       "        user_id                      username reaction_count  likes_standar  \\\n",
       "0  1.296142e+11  LA LUCHA SANGUCHERIA CRIOLLA          121.0       0.008643   \n",
       "1  1.296142e+11  LA LUCHA SANGUCHERIA CRIOLLA           64.0       0.004571   \n",
       "2  1.296142e+11  LA LUCHA SANGUCHERIA CRIOLLA           81.0       0.005786   \n",
       "3  1.296142e+11  LA LUCHA SANGUCHERIA CRIOLLA           85.0       0.006071   \n",
       "4  1.296142e+11  LA LUCHA SANGUCHERIA CRIOLLA          567.0       0.040500   \n",
       "\n",
       "   comments_standar  shares_standar        dia hora  total_reacciones  \n",
       "0          0.003585        0.010323  Wednesday   13          0.034773  \n",
       "1          0.007358        0.006452     Monday   13          0.032418  \n",
       "2          0.002264        0.002581     Friday    9          0.015396  \n",
       "3          0.002075        0.002581     Monday   10          0.015257  \n",
       "4          0.012642        0.051613     Friday   18          0.159266  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Publicaciones2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dadefd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Laecs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Laecs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text:str)->str:\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c01b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(texto):\n",
    "    import emoji\n",
    "\n",
    "    sig_punt = {\n",
    "        ord('\\n'): None,\n",
    "        ord('*'): None,\n",
    "        ord(':'): None,\n",
    "        ord('#'): None,\n",
    "        ord('@'): None,\n",
    "        ord('‚Ä¢'): None\n",
    "    }\n",
    "    \n",
    "    marca = ['laluchasangucheriacriolla','desayunosfit','patriosangucheria','facebook',\n",
    "             '.com','www.','https','.peru','elchinito','delidesayunosd','delidesayunosdelivery',\n",
    "             'laluchasangucheria','eljardindejazmin','desayunodelivery','paraderov',\n",
    "             'elchinovegano','seitanurbanbistro','asianica', 'asianicastreetfood','carnivorolahamburgueser√≠a','carnivorolahamburgueser√≠a',\n",
    "             'lima141','palermocafe','monstruos' ,'sandmonstruos','maztikasanguchesurbanos','patrio','palermo','maztika']\n",
    "    \n",
    "    #If the post have Text\n",
    "    if isinstance(texto, str):\n",
    "        #print(\"texto es: \",texto)\n",
    "        #Remove special characters\n",
    "        texto = texto.translate(sig_punt)\n",
    "        \n",
    "        #Remove emoji\n",
    "        texto = emoji.replace_emoji(texto,'')\n",
    "        \n",
    "        texto = texto.lower()\n",
    "        \n",
    "        #Remove URLs and hashtag words\n",
    "        for i in marca:\n",
    "            texto = texto.replace(i,'')\n",
    "        \n",
    "        #Remove Stopwords\n",
    "        texto = remove_stopwords(texto)\n",
    "        \n",
    "        return texto\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c8dbe",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fab201a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Text transformation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextBert\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(format_text)\n\u001b[0;32m      3\u001b[0m textos \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextBert\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Define BERT Model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Text transformation\n",
    "df['textBert'] = df['text'].apply(format_text)\n",
    "textos = df['textBert']\n",
    "\n",
    "#Define BERT Model\n",
    "topic_model = BERTopic(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", nr_topics='10')\n",
    "\n",
    "#Training with text\n",
    "topics, probs = topic_model.fit_transform(textos)\n",
    "\n",
    "#Reduce outliers\n",
    "new_topics = topic_model.reduce_outliers(textos, topics, strategy=\"distributions\")\n",
    "\n",
    "#Update new topics\n",
    "topic_model.update_topics(textos, topics=new_topics)\n",
    "topic_model.topics_ = new_topics\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": textos, \"Topic\": new_topics})\n",
    "topic_model._update_topic_size(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3d55962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>0_lucha_mejor_hoy_8am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>1_amor_regalospersonalizados_regalosoriginales...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2_menujardinero_paso_campechanito_pastrami</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name\n",
       "0      0    663                              0_lucha_mejor_hoy_8am\n",
       "1      1     79  1_amor_regalospersonalizados_regalosoriginales...\n",
       "2      2     30         2_menujardinero_paso_campechanito_pastrami"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get topics from texts\n",
    "topicos = topic_model.get_document_info(textos)\n",
    "\n",
    "#Concat posts with topics\n",
    "df_topics = pd.concat([df, topicos], axis=1)\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save df\n",
    "df_topics.to_csv(\"Publicaciones_topicos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194740d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
